{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Automation for OpenShift on IBM Power Systems\n\n\nContents:\n\n\n\n\nOCS-UPI-KVM\n\n\nOverview \n\n\nUser Requirements \n\n\nScripts \n\n\nChron tab Automation  \n\n\u00a0\n\n\n\n\n\n\nOCP4-UPI-PowerVS\n\n\nIntroduction \n\n\nAutomation Host Prerequisites \n\n\nPowerVS Prerequisites \n\n\nOCP Install \n\n\u00a0\n\n\n\n\n\n\nOCP4-UPI-KVM\n\n\nIntroduction \n\n\nPrerequisites\n\n\nImage and VM Requirements \n\n\nOCP Install \n\n\u00a0\n\n\n\n\n\n\nOCP4-UPI-PowerVM\n\n\nIntroduction \n\n\nPrerequisites \n\n\nImage and LPAR Requirements \n\n\nOCP Install \n\n\u00a0\n\n\n\n\n\n\nPowerVS_Automation\n\n\u00a0\n\n\nOCP4-Playbooks\n\n\nIntroduction \n\n\nAssumptions \n\n\nBastion and HA Setup \n\n\u00a0\n\n\n\n\n\n\nTerraform-Provider-IBM\n\n\nTerraform Provider \n\n\nRequirements \n\n\nIBM Clouds Ansible Modules",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-automation-for-openshift-on-ibm-power-systems",
            "text": "",
            "title": "Welcome to Automation for OpenShift on IBM Power Systems"
        },
        {
            "location": "/#contents",
            "text": "OCS-UPI-KVM  Overview   User Requirements   Scripts   Chron tab Automation   \n\u00a0    OCP4-UPI-PowerVS  Introduction   Automation Host Prerequisites   PowerVS Prerequisites   OCP Install  \n\u00a0    OCP4-UPI-KVM  Introduction   Prerequisites  Image and VM Requirements   OCP Install  \n\u00a0    OCP4-UPI-PowerVM  Introduction   Prerequisites   Image and LPAR Requirements   OCP Install  \n\u00a0    PowerVS_Automation \n\u00a0  OCP4-Playbooks  Introduction   Assumptions   Bastion and HA Setup  \n\u00a0    Terraform-Provider-IBM  Terraform Provider   Requirements   IBM Clouds Ansible Modules",
            "title": "Contents:"
        },
        {
            "location": "/automation/",
            "text": "OpenShift 4 on PowerVS Automation\n\n\nThis repo contain a bash script which can help you deploy OpenShift Container Platform 4.X on \nIBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud\n.\n\n\nThe script make use of Terraform configurations from \nocp4-upi-powervs\n. Do check out the \nPowerVS Automation\n\n.\n\n\nWhat can the script do\n\n\nIt can run on multiple x86 platforms including where terraform runs out of the box:\n\n\n\n\nMac OSX (Darwin)\n\n\nLinux (x86_64)\n\n\nWindows 10 (Git Bash & Cygwin)\n\n\n\n\nIt can setup the latest IBM Cloud CLI and Terraform for you.\n\n\nIt can help you populate variables for \nocp4-upi-powervs\n via interactive prompts.\n\n\nIt can help you create an OpenShift cluster for you on PowerVS. Thanks to the project \nocp4-upi-powervs\n.\n\n\nHow to use\n\n\nJust create an install directory and download the script on the box.\n\n\ncurl https://raw.githubusercontent.com/ocp-power-automation/powervs_automation/master/deploy.sh -o deploy.sh && chmod +x deploy.sh\n\n\nYou are good to run the script:\n\n\n# ./deploy.sh\n\nAutomation for deploying OpenShift 4.X on PowerVS\n\nUsage:\n  ./deploy.sh [command] [<args> [<value>]]\n\nAvailable commands:\n  setup       Install all required packages/binaries in current directory\n  variables   Interactive way to populate the variables file\n  create      Create an OpenShift cluster\n  destroy     Destroy an OpenShift cluster\n  output      Display the cluster information. Runs terraform output [NAME]\n  help        Display this information\n\nWhere <args>:\n  -trace      Enable tracing of all executed commands\n  -verbose    Enable verbose for terraform console\n  -var        Terraform variable to be passed to the create/destroy command\n  -var-file   Terraform variable file name in current directory. (By default using var.tfvars)\n\nSubmit issues at: https://github.com/ocp-power-automation/ocp4-upi-powervs/issues",
            "title": "Automation"
        },
        {
            "location": "/automation/#openshift-4-on-powervs-automation",
            "text": "This repo contain a bash script which can help you deploy OpenShift Container Platform 4.X on  IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud .  The script make use of Terraform configurations from  ocp4-upi-powervs . Do check out the  PowerVS Automation .",
            "title": "OpenShift 4 on PowerVS Automation"
        },
        {
            "location": "/automation/#what-can-the-script-do",
            "text": "It can run on multiple x86 platforms including where terraform runs out of the box:   Mac OSX (Darwin)  Linux (x86_64)  Windows 10 (Git Bash & Cygwin)   It can setup the latest IBM Cloud CLI and Terraform for you.  It can help you populate variables for  ocp4-upi-powervs  via interactive prompts.  It can help you create an OpenShift cluster for you on PowerVS. Thanks to the project  ocp4-upi-powervs .",
            "title": "What can the script do"
        },
        {
            "location": "/automation/#how-to-use",
            "text": "Just create an install directory and download the script on the box.  curl https://raw.githubusercontent.com/ocp-power-automation/powervs_automation/master/deploy.sh -o deploy.sh && chmod +x deploy.sh  You are good to run the script:  # ./deploy.sh\n\nAutomation for deploying OpenShift 4.X on PowerVS\n\nUsage:\n  ./deploy.sh [command] [<args> [<value>]]\n\nAvailable commands:\n  setup       Install all required packages/binaries in current directory\n  variables   Interactive way to populate the variables file\n  create      Create an OpenShift cluster\n  destroy     Destroy an OpenShift cluster\n  output      Display the cluster information. Runs terraform output [NAME]\n  help        Display this information\n\nWhere <args>:\n  -trace      Enable tracing of all executed commands\n  -verbose    Enable verbose for terraform console\n  -var        Terraform variable to be passed to the create/destroy command\n  -var-file   Terraform variable file name in current directory. (By default using var.tfvars)\n\nSubmit issues at: https://github.com/ocp-power-automation/ocp4-upi-powervs/issues",
            "title": "How to use"
        },
        {
            "location": "/introduction/",
            "text": "Introduction\n\n\nRed Hat and IBM Power teams have been working together on OpenShift since 2018 with the initial release of OpenShift 3.11 on Power. Through this continuous collaboration and effort between IBM and Red Hat, improvements on Power have been made such as the support of IBM PowerVM with OpenShift to enable our Power Enterprise systems. OpenShift on IBM Power Systems takes advantage of the Hybrid Cloud Flexibility, as a result of it, OpenShift can be deployed on PowerVM or Red Hat KVM (development) on Power scale-out servers to exploit the 3.2x container density advantage per core of the POWER9 multi-threaded architecture. Automation for OpenShift on IBM Power Architecture can make things easier and much faster for the end user, making the entire process hasslefree allowing Power clients to exploit the innovative new capabilities.A deployment host is any virtual or physical host that is typically required for the installation of Red Hat OpenShift. The Red Hat OpenShift installation assumes that many, if not all the external services like DNS, load balancing, HTTP server, DHCP are already available in an existing data center and therefore there is no need to duplicate them on a node in the Red Hat OpenShift cluster. Master node, worker node and bootstrap can run on top of PowerVC, PowerVM, Red Hat Virtualization, KVM or run bare metal environments. You can manage trust configuration directly on each node or manage the files on a separate host by distributing them to the appropriate nodes using Ansible.\n\n\nAnsible is an automation tool used to configure systems, deploy software, and perform rolling updates. Ansible includes support for container-native virtualization, and Ansible modules enable you to automate cluster management tasks such as template, persistent volume claim, and virtual machine operations. Ansible provides a way to automate container-native virtualization management, which you can also accomplish by using the oc CLI tool or APIs. Ansible is unique because it allows you to integrate KubeVirt modules with other Ansible modules.\n\n\n\n\nPlaybooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material.\nAt a basic level, playbooks can be used to manage configurations of and deployments to remote machines. At a more advanced level, they can sequence multi-tier rollouts involving rolling updates, and can delegate actions to other hosts, interacting with monitoring servers and load balancers along the way.\nPlaybooks are designed to be human-readable and are developed in a basic text language. There are multiple ways to organize playbooks and the files they include.\nThe Playbooks are used for installation of OCP on Power and other post install customizations\n\n\nTerraform is an Open Source software that is developed by HashiCorp that enables predictable and consistent provisioning of IBM Cloud platform, classic infrastructure, and VPC infrastructure resources by using a high-level scripting language. Terraform is used to create, manage, and update infrastructure resources such as physical machines, VMs, network switches, containers, and more. Terraform can be used to automate IBM Cloud resource provisioning, rapidly build complex, multi-tier cloud environments, and enable Infrastructure as Code (IaC).\n\n\nA lot of OpenShift 4 specific jargon is used throughout this doc, so please visit the official documentation page to get familiar with OpenShift 4.",
            "title": "Introduction"
        },
        {
            "location": "/introduction/#introduction",
            "text": "Red Hat and IBM Power teams have been working together on OpenShift since 2018 with the initial release of OpenShift 3.11 on Power. Through this continuous collaboration and effort between IBM and Red Hat, improvements on Power have been made such as the support of IBM PowerVM with OpenShift to enable our Power Enterprise systems. OpenShift on IBM Power Systems takes advantage of the Hybrid Cloud Flexibility, as a result of it, OpenShift can be deployed on PowerVM or Red Hat KVM (development) on Power scale-out servers to exploit the 3.2x container density advantage per core of the POWER9 multi-threaded architecture. Automation for OpenShift on IBM Power Architecture can make things easier and much faster for the end user, making the entire process hasslefree allowing Power clients to exploit the innovative new capabilities.A deployment host is any virtual or physical host that is typically required for the installation of Red Hat OpenShift. The Red Hat OpenShift installation assumes that many, if not all the external services like DNS, load balancing, HTTP server, DHCP are already available in an existing data center and therefore there is no need to duplicate them on a node in the Red Hat OpenShift cluster. Master node, worker node and bootstrap can run on top of PowerVC, PowerVM, Red Hat Virtualization, KVM or run bare metal environments. You can manage trust configuration directly on each node or manage the files on a separate host by distributing them to the appropriate nodes using Ansible.  Ansible is an automation tool used to configure systems, deploy software, and perform rolling updates. Ansible includes support for container-native virtualization, and Ansible modules enable you to automate cluster management tasks such as template, persistent volume claim, and virtual machine operations. Ansible provides a way to automate container-native virtualization management, which you can also accomplish by using the oc CLI tool or APIs. Ansible is unique because it allows you to integrate KubeVirt modules with other Ansible modules.   Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material.\nAt a basic level, playbooks can be used to manage configurations of and deployments to remote machines. At a more advanced level, they can sequence multi-tier rollouts involving rolling updates, and can delegate actions to other hosts, interacting with monitoring servers and load balancers along the way.\nPlaybooks are designed to be human-readable and are developed in a basic text language. There are multiple ways to organize playbooks and the files they include.\nThe Playbooks are used for installation of OCP on Power and other post install customizations  Terraform is an Open Source software that is developed by HashiCorp that enables predictable and consistent provisioning of IBM Cloud platform, classic infrastructure, and VPC infrastructure resources by using a high-level scripting language. Terraform is used to create, manage, and update infrastructure resources such as physical machines, VMs, network switches, containers, and more. Terraform can be used to automate IBM Cloud resource provisioning, rapidly build complex, multi-tier cloud environments, and enable Infrastructure as Code (IaC).  A lot of OpenShift 4 specific jargon is used throughout this doc, so please visit the official documentation page to get familiar with OpenShift 4.",
            "title": "Introduction"
        },
        {
            "location": "/ocp_kvm/",
            "text": "Table of Contents\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nPre-requisites\n\n\nImage and VM requirements\n\n\nOCP Install\n\n\nContributing\n\n\n\n\nIntroduction\n\n\nThis repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using\nlibvirt.\n\n\nThis project leverages the \nfollowing ansible playbook\n to setup\nhelper node (bastion) for OCP deployment.\n\n\nRun this code from either Mac or Linux (Intel) system. For automating setup, see\n\nOCP-UPI-KVM\n\n\n:heavy_exclamation_mark: \nThis automation is intended for test/development purposes only and there is no formal support. For issues please open a GitHub issue\n\n\nPre-requisites\n\n\n\n\nGit\n: Please refer to the following \nlink\n for instructions\non installing \ngit\n for Linux and Mac.\n\n\nTerraform >= 0.13\n: Please refer to the following \nlink\n for instructions on installing \nterraform\n for Linux and Mac. For validating the version run \nterraform version\n command after install.\n\n\nTerraform Providers\n: Please ensure terraform providers are built and installed on Terraform Client Machine. You can follow the \nBuild Terraform Providers\n guide.\n\n\nlibvirt\n: Please ensure \nlibvirt\n is installed and configured on the KVM host. You can follow the \nLibvirt Host setup\n guide.\n\n\n\n\nImage and VM requirements\n\n\nFor information on how to configure the images required for the automation see \nPreparing Images for Power\n.\n\n\nFollowing are the recommended VM configs for OpenShift nodes that will be deployed with RHCOS image.\n- Bootstrap, Master - 4 vCPUs, 16GB RAM, 120 GB Disk\n\n\nThis config is suitable for majority of the scenarios\n\n- Worker - 4 vCPUs, 16GB RAM, 120 GB Disk\n\n\nIncrease worker vCPUs, RAM and Disk based on application requirements\n\n\nFollowing is the recommended VM config for the helper node that will be deployed with RHEL 8.0 (or later) image.\n- Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk\n\n\nOCP Install\n\n\nFollow these \nquickstart\n steps to kickstart OCP installation on Power KVM using libvirt.\n\n\nContributing\n\n\nPlease see the \ncontributing doc\n for more details.\nPRs are most welcome !!",
            "title": "Ocp kvm"
        },
        {
            "location": "/ocp_kvm/#table-of-contents",
            "text": "Table of Contents  Introduction  Pre-requisites  Image and VM requirements  OCP Install  Contributing",
            "title": "Table of Contents"
        },
        {
            "location": "/ocp_kvm/#introduction",
            "text": "This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using\nlibvirt.  This project leverages the  following ansible playbook  to setup\nhelper node (bastion) for OCP deployment.  Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-KVM  :heavy_exclamation_mark:  This automation is intended for test/development purposes only and there is no formal support. For issues please open a GitHub issue",
            "title": "Introduction"
        },
        {
            "location": "/ocp_kvm/#pre-requisites",
            "text": "Git : Please refer to the following  link  for instructions\non installing  git  for Linux and Mac.  Terraform >= 0.13 : Please refer to the following  link  for instructions on installing  terraform  for Linux and Mac. For validating the version run  terraform version  command after install.  Terraform Providers : Please ensure terraform providers are built and installed on Terraform Client Machine. You can follow the  Build Terraform Providers  guide.  libvirt : Please ensure  libvirt  is installed and configured on the KVM host. You can follow the  Libvirt Host setup  guide.",
            "title": "Pre-requisites"
        },
        {
            "location": "/ocp_kvm/#image-and-vm-requirements",
            "text": "For information on how to configure the images required for the automation see  Preparing Images for Power .  Following are the recommended VM configs for OpenShift nodes that will be deployed with RHCOS image.\n- Bootstrap, Master - 4 vCPUs, 16GB RAM, 120 GB Disk  This config is suitable for majority of the scenarios \n- Worker - 4 vCPUs, 16GB RAM, 120 GB Disk  Increase worker vCPUs, RAM and Disk based on application requirements  Following is the recommended VM config for the helper node that will be deployed with RHEL 8.0 (or later) image.\n- Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk",
            "title": "Image and VM requirements"
        },
        {
            "location": "/ocp_kvm/#ocp-install",
            "text": "Follow these  quickstart  steps to kickstart OCP installation on Power KVM using libvirt.",
            "title": "OCP Install"
        },
        {
            "location": "/ocp_kvm/#contributing",
            "text": "Please see the  contributing doc  for more details.\nPRs are most welcome !!",
            "title": "Contributing"
        },
        {
            "location": "/ocs_kvm/",
            "text": "Overview\n\n\nProvide scripts enabling the automation of OCS-CI on IBM Power Servers,\nincluding the ability to create an OCP cluster, run OCS-CI tests, and\ndestroy OCS (and OCP).  OCS-CI provides a deployment option to install\nOpenShift Container Storage on the given worker nodes based on\nRed Hat Ceph Storage.\n\n\nParameters related to the definition of the cluster such as the\nOpenShift Version and the number and size of worker nodes are specified\nvia environment variables to the scripts listed below.\n\n\nThe goal of this project is to provide the primitives that are needed\nto enable OCS-CI on Power Servers.  These scripts form a framework for the\ndevelopment and validation of OpenShift clusters and operators.\n\n\nThis project utilizes KVM to create a OpenShift Cluster running in VMs.  This\nproject runs on baremetal servers as well as PowerVM and PowerVS based servers\nprovided that a large enough LPAR is allocated. For automating setup, see\n\nOCS-UPI-KVM\n\n\nUser Requirements\n\n\nThis project may be run by non-root users with \nsudo\n authority.\n\npasswordless\n sudo access should be enabled, so that the scripts\ndon't prompt for a password.  A helper script is provided\nfor this purpose at \nscripts/helper/set-passwordless-sudo.sh\n.  There are no\ncommand arguments for this script and it should be run once during initial setup.\n\n\nNote: Non-root users must use the \nsudo\n command with \nvirsh\n to see VMs\n\n\nScripts\n\n\nThe scripts below correspond to high level tasks of OCS-CI.  They are intended to\nbe invoked from an automation test framework such as \nJenkins\n.\nThe scripts are listed in the order that they are expected to be run.\n\n\n\n\ncreate-ocp.sh [ --retry ]\n\n\nsetup-ocs-ci.sh\n\n\ndeploy-ocs-ci.sh\n\n\ntest-ocs-ci.sh [ --tier <0,1,...> ]\n\n\nteardown-ocs-ci.sh\n\n\ndestroy-ocp.sh\n\n\n\n\nThis project uses the following git submodules:\n\n\n\n\ngithub.com/ocp-power-automation/ocp4-upi-kvm \n\n\ngithub.com/red-hat-storage/ocs-ci\n\n\n\n\nThese underlying projects must be instantiated before the create, setup, deploy,\ntest, and teardown scripts are used.  The user is expected to setup the submodules\nbefore invoking these scripts.  The \nworkflow sample\n scripts described in the next section\nprovide some end to end work flows which of necessity instantiate submodules. These\nsample scripts may be copied to the workspace directory and edited as desired to\ncustomize a work flow.  Most users are expected to do this.  The information\nprovided below describes some of the dynamics surrounding the create, deploy,\nand test scripts.\n\n\nFirst, there are two ways to instantiate submodules as shown below:\n\n\ngit clone https://github.com/ocp-power-automation/ocs-upi-kvm --recursive\ncd ocs-upi-kvm/scripts\n\n\n\n\nOR\n\n\ngit clone https://github.com/ocp-power-automation/ocs-upi-kvm.sh\ncd ocs-upi-kvm\ngit submodule update --init\n\n\n\n\nThe majority of the \ncreate-ocp.sh\n command is spent running terraform (and ansible).\nOn occasion, a transient error will occur while creating the cluster.  In this case,\nthe operation can be restarted by specifying the  \n--retry\n argument.  This can save\nhalf an hour of execution time.  If this argument is not specified, the existing\ncluster will be torn down automatically assuming there is one.\n\n\nIf a failure occurs while running the \ndeploy-ocs-ci.sh\n script, the operation has to be\nrestarted from the beginning.  That is to say with \ncreat-ocp.sh\n.  Do not specify\nthe --retry argument as the OCP cluster has to be completely removed before trying to deploy\nOCS.  The ocs-ci project alters the state of the OCP cluster.\n\n\nFurther, the \nteardown-ocs-ci.sh\n script has never been obcserved to work cleanly.  This\nsimply invokes the underlying ocs-ci function.  It is provided as it may be fixed in time\nand it is a valuable function, if only in theory now.\n\n\nThe script \ndestroy-ocp.sh\n which recompletely removes ocp and ocs.\n\n\nThe script \ncreate-ocp.sh\n will also remove an existing OCP cluster if one is present\nbefore creating a new one as \nonly one OCP cluster is supported on the host KVM server\nat a time\n.  This is true even if the cluster was created by another user, so if you are\nconcerned with impacting other users run this command first, \nsudo virsh list --all\n.\n\n\nWorkflow Sample Scripts\n\n\nsamples/dev-ocs.sh [--retry-ocp] [--latest-ocs] \nsamples/test-ocs.sh [--retry-ocp] [--latest-ocs]\n\n\n\n\nThese scripts are useful in getting started.  They implement the full sequence of\nhigh level tasks defined above.  The \ntest-ocs.sh\n invokes \nocs-ci\n tier tests\n2, 3, 4, 4a, 4b, and 4c.  Both scripts designate the use of file backed Ceph disks\nwhich are based on qcow2 files.  These files are sparsely populated enabling the \nuse of servers with as little as 256 GBs of storage, depending on the number of \nworker nodes that are requested.  The use of file backed data disks is the default.\nThe test-ocs scripts include comments showing how physical disk partitions may\nbe used instead which may improve performance and resilience.\n\n\nAs noted above, these scripts may be relocated, customized, and invoked from the\n\nworkspace\n directory.\n\n\nRequired Environment Variables\n\n\n\n\nRHID_USERNAME=xxx\n\n\nRHID_PASSWORD=yyy\n\n\n\n\nOR\n\n\n\n\nRHID_ORG=ppp\n\n\nRHID_KEY=qqq\n\n\n\n\nProject Workspace\n\n\nThis project is designed to be used in an automated test framework like Jenkins which utilizes\ndedicated workspaces to run jobs in parallel on the same server.  As such, all input, output,\nand internally generated files are restricted to the specific workspace instance that\nis allocated to run the job.  For this project, that workspace is assumed to be \nthe \nparent\n directory of the cloned project \nocs-upi-kvm\n itself.\n\n\nRequired Files\n\n\nThese files should be placed in the workspace directory.\n\n\n\n\n~/auth.yaml\n\n\n~/pull-secret.txt\n\n\n~/$BASTION_IMAGE\n\n\n\n\nThe auth.yaml file is required for the script deploy-ocs-ci.sh.  It contains secrets\nfor \nquay\n and \nquay.io/rhceph-dev\n which are obtained from the Redhat OCS-CI team.\n\n\nThe pull-secret.txt is required for the scripts create-ocp.sh and deploy-ocs-ci.sh.\nDownload your managed pull secrets from https://cloud.redhat.com/openshift/install/pull-secret and add\nthe secret for \nquay.io/rhceph-dev\n noted above to the pull-secret.txt file.  You will\nalso need to add the secret for \nregistry.svc.ci.openshift.org\n which may be obtained as follows:\n\n\n\n\nBecome a member of \nopenshift organization\n\n\nlogin to https://api.ci.openshift.org/console/catalog\n\n\nClick on \"copy login command\" under username in the right corner. (This will copy the oc login command to your clipboard.)\n\n\nNow open your terminal, paste from the clipboard buffer and execute that command you just pasted (oc login).\n\n\nExecute the oc registry login --registry registry.svc.ci.openshift.org which will store your token in ~/.docker/config.json.\n\n\n\n\nThe bastion image is a prepared image downloaded from the Red Hat Customer Portal following these\n\ninstructions\n.\nIt is named by the environment variable BASTION_IMAGE which has a default\nvalue of \"rhel-8.2-update-2-ppc64le-kvm.qcow2\".  This is the name of the file downloaded\nfrom the RedHat website.\n\n\nWhen preparing the bastion image above, the root password must be set to \n123456\n.\n\n\nOptional Environment Variables with Default Values\n\n\n\n\nOCP_VERSION=${OCP_VERSION:=\"4.5\"}\n\n\nCLUSTER_DOMAIN=${CLUSTER_DOMAIN:=\"tt.testing\"}\n\n\nBASTION_IMAGE=${BASTION_IMAGE:=\"rhel-8.2-update-2-ppc64le-kvm.qcow2\"}\n\n\nMASTER_DESIRED_CPU=${MASTER_DESIRED_CPU:=\"4\"}\n\n\nMASTER_DESIRED_MEM=${MASTER_DESIRED_MEM:=\"16384\"}\n\n\nWORKER_DESIRED_CPU=${WORKER_DESIRED_CPU:=\"16\"}\n\n\nWORKER_DESIRED_MEM=${WORKER_DESIRED_MEM:=\"65536\"}\n\n\nWORKERS=${WORKERS:=3}\n\n\nIMAGES_PATH=${IMAGES_PATH:=\"/var/lib/libvirt/images\"}\n\n\nOPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=${OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE:=\"\"}\n\n\nDATA_DISK_SIZE=${DATA_DISK_SIZE:=256}\n\n\nDATA_DISK_LIST=${DATA_DISK_LIST:=\"\"}\n\n\nFORCE_DISK_PARTITION_WIPE=${FORCE_DISK_PARTITION_WIPE:=\"false\"}\n\n\nCHRONY_CONFIG=${CHRONY_CONFIG:=\"true\"}\n\n\nRHCOS_RELEASE=${RHCOS_RELEASE:=\"\"}\n\n\n\n\nDisk sizes are in GBs.\n\n\nThe \nCHRONY_CONFIG\n parameter above enables NTP servers as OCS CI expects them\nto be configured.  If that is not applicable, then this parameter should probably\nbe set to false.\n\n\nThe \nRHCOS_RELEASE\n parameter is specific to the \nOCP_VERSION\n and is set internally\nto the \nlatest available \nrhcos\n image available\n\nprovided that it is not set by the user.  The internal setting may be outdated.\n\n\nThe supported \nOCP_VERSION\ns are 4.4 - 4.7.\n\n\nSet a new value like this:\n\n\nexport OCP_VERSION=4.6\nexport RHCOS_RELEASE=4.6.1\n\n\n\n\nThe OpenShift installer uses the latest available image which by default is\na development build.  For released OCP versions, this tool will chose a recently\nreleased image based on the environment OCP_VERSION.  This image may not be\nlatest available version.  This internal selection may be overriden by setting\nthe environment variable \nOPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE\n\nto a \ndevelopment preview image\n.\n\n\nSet a specific daily build like this:\n\n\nREGISTRY=\"registry.svc.ci.openshift.org/ocp-ppc64le/release-ppc64le\"\nexport OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=\"$REGISTRY:4.6.0-0.nightly-ppc64le-2020-09-27-075246\"\n\n\n\n\nThe script \ncreate-ocp.sh\n will add a data disk to each worker node.  This disk is presented\ninside the worker node as /dev/vdc.  In the KVM host server OS, the data disk is backed \nby either a file or a physical disk partition.  If you specify the environment\nvariable DATA_DISK_LIST, then the named physical disk partitions (/dev) will be used.\nThe list is composed of comma separated unique partition names with one partition name\nspecified per worker node. For example,\n\n\nexport DATA_DISK_LIST=\"sdi1,sdi2,sdi3\"\n\n\n\n\nOtherwise, the data disks will be backed by a file.  The environment variable\nDATA_DISK_SIZE controls the size of the file allocation.  If you don't want the \nextra disk to be allocated, then set DATA_DISK_SIZE=0.  In this case, don't run\nthe scripts \nsetup-ocs-ci.sh\n or \ndeploy-ocs-ci.sh\n as they will fail.\n\n\nThe environment variable FORCE_DISK_PARTITION_WIPE may be set to 'true' to wipe\nthe data on a hard disk partition assuming the environment variable DATA_DISK_LIST is\nspecified.  The wipe may take an hour or more to complete.\n\n\nPost Creation of The OpenShift Cluster\n\n\nBuild artifacts are placed in the \nworkspace\n directory which is defined as the\nparent directory of this github project \nocs-upi-kvm\n.  The examples shown below\nuse a dedicated directory for this purpose as there are quite a few output\nfiles, some of which are not shown below such as rpms and tar files that are\ndownloaded during cluster creation.\n\n\nThe \noc\n Command\n\n\nUpon successful completion of the \ncreate-ocp.sh\n script, the \noc\n command\nmay be invoked in the following way:\n\n\n[user@kvm-host workspace]; source env-ocp.sh\n[user@kvm-host workspace]; oc get nodes\nNAME       STATUS   ROLES    AGE   VERSION\nmaster-0   Ready    master   40h   v1.19.0+d59ce34\nmaster-1   Ready    master   40h   v1.19.0+d59ce34\nmaster-2   Ready    master   40h   v1.19.0+d59ce34\nworker-0   Ready    worker   39h   v1.19.0+d59ce34\nworker-1   Ready    worker   39h   v1.19.0+d59ce34\nworker-2   Ready    worker   39h   v1.19.0+d59ce34\n\n\n\n\nThe \nenv-ocp.sh\n script exports \nKUBECONFIG\n and updates the \nPATH\n environment\nvariable.  It may be useful in some cases to stick these in your user profile.\n\n\nLog Files\n\n\nThe following log files are produced:\n\n\n[user@kvm-host-ahv workspace]$ ls -lt *log\n-rw-rw-r--. 1 luke luke   409491 Oct 23 18:36 create-ocp.log\n-rw-rw-r--. 1 luke luke   654998 Oct 23 19:06 deploy-ocs-ci.log\n-rw-rw-r--. 1 luke luke  1144731 Oct 22 23:23 ocs-ci.git.log\n-rw-rw-r--. 1 luke luke    18468 Oct 23 18:38 setup-ocs-ci.log\n-rw-rw-r--. 1 luke luke 23431620 Oct 23 18:35 terraform.log\n-rw-rw-r--. 1 luke luke 29235845 Oct 25 00:30 test-ocs-ci.log\n\n\n\n\nRemote Webconsole Support\n\n\nThe cluster create command outputs webconsole information which should look something\nlike the first entry below.  This information needs to be added to your /etc/hosts file\nor MacOS equivalent defining the IP Address of the KVM host server.  You must generate\nthe companion \noauth\n definition as shown below following the same pattern.\n\n\n<ip kvm host server> console-openshift-console.apps.test-ocp4-5.tt.testing oauth-openshift.apps.test-ocp4-5.tt.testing\n\n\n\n\nThe browser should prompt you to login to the OCP cluster.  The user name is \nkubeadmin\n and\nthe password is located in the file \n/auth/kubeadmin-password\n.\n\n\nChrontab Automation\n\n\nThe following two files have been provided:\n\n\n\n\nchron-ocs.sh \n\n\ntest-chron-ocs.sh\n\n\n\n\nThe \nchron-ocs.sh\n script is the master chrontab commandline script.  It is located\nin the \nscripts/helper\n directory.\n\n\nThe \ntest-chron-ocs.sh\n script is invoked by chron-ocs.sh and provides the\nend-to-end OCP/OCS command flow.  Presently, this script invokes tier tests\n2, 3, 4, 4a, 4b and 4c.  You can limit the tests to a subset by editing this file.\nThis file is located in the \nsamples\n directory. \n\n\nTo setup chrontab automation, you must:\n\n\n\n\nCreate \ntest\n user account with sudo authority and login to it\n\n\ngit clone this project in $HOME and invoke \nscripts/helper/set-passwordless-sudo.sh\n\n\nPlace the required files defined by the ocs-upi-kvm project in $HOME\n\n\nCopy the two chron scripts listed above to $HOME\n\n\nEdit the four lines below in \ntest-chron-ocs.sh\n:\n\n\n\n\nexport RHID_USERNAME=<Your RedHat Subscription id>\nexport RHID_PASSWORD=<RedHat Subscription password>\nexport OCP_VERSION=4.5\nexport IMAGES_PATH=/home/libvirt/images\n\n\n\n\n\n\nInvoke \ncrontab -e\n and enter the following two lines:\n\n\n\n\nSHELL=/bin/bash \n0 0 * * * ~/chron-ocs.sh\n\n\n\n\nThe example above will invoke chron-ocs.sh every 24 hours at midnight local time.\n\n\nLog files are written to the \nlogs\n directory under the user's home directory.",
            "title": "Ocs kvm"
        },
        {
            "location": "/ocs_kvm/#overview",
            "text": "Provide scripts enabling the automation of OCS-CI on IBM Power Servers,\nincluding the ability to create an OCP cluster, run OCS-CI tests, and\ndestroy OCS (and OCP).  OCS-CI provides a deployment option to install\nOpenShift Container Storage on the given worker nodes based on\nRed Hat Ceph Storage.  Parameters related to the definition of the cluster such as the\nOpenShift Version and the number and size of worker nodes are specified\nvia environment variables to the scripts listed below.  The goal of this project is to provide the primitives that are needed\nto enable OCS-CI on Power Servers.  These scripts form a framework for the\ndevelopment and validation of OpenShift clusters and operators.  This project utilizes KVM to create a OpenShift Cluster running in VMs.  This\nproject runs on baremetal servers as well as PowerVM and PowerVS based servers\nprovided that a large enough LPAR is allocated. For automating setup, see OCS-UPI-KVM",
            "title": "Overview"
        },
        {
            "location": "/ocs_kvm/#user-requirements",
            "text": "This project may be run by non-root users with  sudo  authority. passwordless  sudo access should be enabled, so that the scripts\ndon't prompt for a password.  A helper script is provided\nfor this purpose at  scripts/helper/set-passwordless-sudo.sh .  There are no\ncommand arguments for this script and it should be run once during initial setup.  Note: Non-root users must use the  sudo  command with  virsh  to see VMs",
            "title": "User Requirements"
        },
        {
            "location": "/ocs_kvm/#scripts",
            "text": "The scripts below correspond to high level tasks of OCS-CI.  They are intended to\nbe invoked from an automation test framework such as  Jenkins .\nThe scripts are listed in the order that they are expected to be run.   create-ocp.sh [ --retry ]  setup-ocs-ci.sh  deploy-ocs-ci.sh  test-ocs-ci.sh [ --tier <0,1,...> ]  teardown-ocs-ci.sh  destroy-ocp.sh   This project uses the following git submodules:   github.com/ocp-power-automation/ocp4-upi-kvm   github.com/red-hat-storage/ocs-ci   These underlying projects must be instantiated before the create, setup, deploy,\ntest, and teardown scripts are used.  The user is expected to setup the submodules\nbefore invoking these scripts.  The  workflow sample  scripts described in the next section\nprovide some end to end work flows which of necessity instantiate submodules. These\nsample scripts may be copied to the workspace directory and edited as desired to\ncustomize a work flow.  Most users are expected to do this.  The information\nprovided below describes some of the dynamics surrounding the create, deploy,\nand test scripts.  First, there are two ways to instantiate submodules as shown below:  git clone https://github.com/ocp-power-automation/ocs-upi-kvm --recursive\ncd ocs-upi-kvm/scripts  OR  git clone https://github.com/ocp-power-automation/ocs-upi-kvm.sh\ncd ocs-upi-kvm\ngit submodule update --init  The majority of the  create-ocp.sh  command is spent running terraform (and ansible).\nOn occasion, a transient error will occur while creating the cluster.  In this case,\nthe operation can be restarted by specifying the   --retry  argument.  This can save\nhalf an hour of execution time.  If this argument is not specified, the existing\ncluster will be torn down automatically assuming there is one.  If a failure occurs while running the  deploy-ocs-ci.sh  script, the operation has to be\nrestarted from the beginning.  That is to say with  creat-ocp.sh .  Do not specify\nthe --retry argument as the OCP cluster has to be completely removed before trying to deploy\nOCS.  The ocs-ci project alters the state of the OCP cluster.  Further, the  teardown-ocs-ci.sh  script has never been obcserved to work cleanly.  This\nsimply invokes the underlying ocs-ci function.  It is provided as it may be fixed in time\nand it is a valuable function, if only in theory now.  The script  destroy-ocp.sh  which recompletely removes ocp and ocs.  The script  create-ocp.sh  will also remove an existing OCP cluster if one is present\nbefore creating a new one as  only one OCP cluster is supported on the host KVM server\nat a time .  This is true even if the cluster was created by another user, so if you are\nconcerned with impacting other users run this command first,  sudo virsh list --all .",
            "title": "Scripts"
        },
        {
            "location": "/ocs_kvm/#workflow-sample-scripts",
            "text": "samples/dev-ocs.sh [--retry-ocp] [--latest-ocs] \nsamples/test-ocs.sh [--retry-ocp] [--latest-ocs]  These scripts are useful in getting started.  They implement the full sequence of\nhigh level tasks defined above.  The  test-ocs.sh  invokes  ocs-ci  tier tests\n2, 3, 4, 4a, 4b, and 4c.  Both scripts designate the use of file backed Ceph disks\nwhich are based on qcow2 files.  These files are sparsely populated enabling the \nuse of servers with as little as 256 GBs of storage, depending on the number of \nworker nodes that are requested.  The use of file backed data disks is the default.\nThe test-ocs scripts include comments showing how physical disk partitions may\nbe used instead which may improve performance and resilience.  As noted above, these scripts may be relocated, customized, and invoked from the workspace  directory.",
            "title": "Workflow Sample Scripts"
        },
        {
            "location": "/ocs_kvm/#required-environment-variables",
            "text": "RHID_USERNAME=xxx  RHID_PASSWORD=yyy   OR   RHID_ORG=ppp  RHID_KEY=qqq",
            "title": "Required Environment Variables"
        },
        {
            "location": "/ocs_kvm/#project-workspace",
            "text": "This project is designed to be used in an automated test framework like Jenkins which utilizes\ndedicated workspaces to run jobs in parallel on the same server.  As such, all input, output,\nand internally generated files are restricted to the specific workspace instance that\nis allocated to run the job.  For this project, that workspace is assumed to be \nthe  parent  directory of the cloned project  ocs-upi-kvm  itself.",
            "title": "Project Workspace"
        },
        {
            "location": "/ocs_kvm/#required-files",
            "text": "These files should be placed in the workspace directory.   ~/auth.yaml  ~/pull-secret.txt  ~/$BASTION_IMAGE   The auth.yaml file is required for the script deploy-ocs-ci.sh.  It contains secrets\nfor  quay  and  quay.io/rhceph-dev  which are obtained from the Redhat OCS-CI team.  The pull-secret.txt is required for the scripts create-ocp.sh and deploy-ocs-ci.sh.\nDownload your managed pull secrets from https://cloud.redhat.com/openshift/install/pull-secret and add\nthe secret for  quay.io/rhceph-dev  noted above to the pull-secret.txt file.  You will\nalso need to add the secret for  registry.svc.ci.openshift.org  which may be obtained as follows:   Become a member of  openshift organization  login to https://api.ci.openshift.org/console/catalog  Click on \"copy login command\" under username in the right corner. (This will copy the oc login command to your clipboard.)  Now open your terminal, paste from the clipboard buffer and execute that command you just pasted (oc login).  Execute the oc registry login --registry registry.svc.ci.openshift.org which will store your token in ~/.docker/config.json.   The bastion image is a prepared image downloaded from the Red Hat Customer Portal following these instructions .\nIt is named by the environment variable BASTION_IMAGE which has a default\nvalue of \"rhel-8.2-update-2-ppc64le-kvm.qcow2\".  This is the name of the file downloaded\nfrom the RedHat website.  When preparing the bastion image above, the root password must be set to  123456 .",
            "title": "Required Files"
        },
        {
            "location": "/ocs_kvm/#optional-environment-variables-with-default-values",
            "text": "OCP_VERSION=${OCP_VERSION:=\"4.5\"}  CLUSTER_DOMAIN=${CLUSTER_DOMAIN:=\"tt.testing\"}  BASTION_IMAGE=${BASTION_IMAGE:=\"rhel-8.2-update-2-ppc64le-kvm.qcow2\"}  MASTER_DESIRED_CPU=${MASTER_DESIRED_CPU:=\"4\"}  MASTER_DESIRED_MEM=${MASTER_DESIRED_MEM:=\"16384\"}  WORKER_DESIRED_CPU=${WORKER_DESIRED_CPU:=\"16\"}  WORKER_DESIRED_MEM=${WORKER_DESIRED_MEM:=\"65536\"}  WORKERS=${WORKERS:=3}  IMAGES_PATH=${IMAGES_PATH:=\"/var/lib/libvirt/images\"}  OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=${OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE:=\"\"}  DATA_DISK_SIZE=${DATA_DISK_SIZE:=256}  DATA_DISK_LIST=${DATA_DISK_LIST:=\"\"}  FORCE_DISK_PARTITION_WIPE=${FORCE_DISK_PARTITION_WIPE:=\"false\"}  CHRONY_CONFIG=${CHRONY_CONFIG:=\"true\"}  RHCOS_RELEASE=${RHCOS_RELEASE:=\"\"}   Disk sizes are in GBs.  The  CHRONY_CONFIG  parameter above enables NTP servers as OCS CI expects them\nto be configured.  If that is not applicable, then this parameter should probably\nbe set to false.  The  RHCOS_RELEASE  parameter is specific to the  OCP_VERSION  and is set internally\nto the  latest available  rhcos  image available \nprovided that it is not set by the user.  The internal setting may be outdated.  The supported  OCP_VERSION s are 4.4 - 4.7.  Set a new value like this:  export OCP_VERSION=4.6\nexport RHCOS_RELEASE=4.6.1  The OpenShift installer uses the latest available image which by default is\na development build.  For released OCP versions, this tool will chose a recently\nreleased image based on the environment OCP_VERSION.  This image may not be\nlatest available version.  This internal selection may be overriden by setting\nthe environment variable  OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE \nto a  development preview image .  Set a specific daily build like this:  REGISTRY=\"registry.svc.ci.openshift.org/ocp-ppc64le/release-ppc64le\"\nexport OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE=\"$REGISTRY:4.6.0-0.nightly-ppc64le-2020-09-27-075246\"  The script  create-ocp.sh  will add a data disk to each worker node.  This disk is presented\ninside the worker node as /dev/vdc.  In the KVM host server OS, the data disk is backed \nby either a file or a physical disk partition.  If you specify the environment\nvariable DATA_DISK_LIST, then the named physical disk partitions (/dev) will be used.\nThe list is composed of comma separated unique partition names with one partition name\nspecified per worker node. For example,  export DATA_DISK_LIST=\"sdi1,sdi2,sdi3\"  Otherwise, the data disks will be backed by a file.  The environment variable\nDATA_DISK_SIZE controls the size of the file allocation.  If you don't want the \nextra disk to be allocated, then set DATA_DISK_SIZE=0.  In this case, don't run\nthe scripts  setup-ocs-ci.sh  or  deploy-ocs-ci.sh  as they will fail.  The environment variable FORCE_DISK_PARTITION_WIPE may be set to 'true' to wipe\nthe data on a hard disk partition assuming the environment variable DATA_DISK_LIST is\nspecified.  The wipe may take an hour or more to complete.",
            "title": "Optional Environment Variables with Default Values"
        },
        {
            "location": "/ocs_kvm/#post-creation-of-the-openshift-cluster",
            "text": "Build artifacts are placed in the  workspace  directory which is defined as the\nparent directory of this github project  ocs-upi-kvm .  The examples shown below\nuse a dedicated directory for this purpose as there are quite a few output\nfiles, some of which are not shown below such as rpms and tar files that are\ndownloaded during cluster creation.",
            "title": "Post Creation of The OpenShift Cluster"
        },
        {
            "location": "/ocs_kvm/#the-oc-command",
            "text": "Upon successful completion of the  create-ocp.sh  script, the  oc  command\nmay be invoked in the following way:  [user@kvm-host workspace]; source env-ocp.sh\n[user@kvm-host workspace]; oc get nodes\nNAME       STATUS   ROLES    AGE   VERSION\nmaster-0   Ready    master   40h   v1.19.0+d59ce34\nmaster-1   Ready    master   40h   v1.19.0+d59ce34\nmaster-2   Ready    master   40h   v1.19.0+d59ce34\nworker-0   Ready    worker   39h   v1.19.0+d59ce34\nworker-1   Ready    worker   39h   v1.19.0+d59ce34\nworker-2   Ready    worker   39h   v1.19.0+d59ce34  The  env-ocp.sh  script exports  KUBECONFIG  and updates the  PATH  environment\nvariable.  It may be useful in some cases to stick these in your user profile.",
            "title": "The oc Command"
        },
        {
            "location": "/ocs_kvm/#log-files",
            "text": "The following log files are produced:  [user@kvm-host-ahv workspace]$ ls -lt *log\n-rw-rw-r--. 1 luke luke   409491 Oct 23 18:36 create-ocp.log\n-rw-rw-r--. 1 luke luke   654998 Oct 23 19:06 deploy-ocs-ci.log\n-rw-rw-r--. 1 luke luke  1144731 Oct 22 23:23 ocs-ci.git.log\n-rw-rw-r--. 1 luke luke    18468 Oct 23 18:38 setup-ocs-ci.log\n-rw-rw-r--. 1 luke luke 23431620 Oct 23 18:35 terraform.log\n-rw-rw-r--. 1 luke luke 29235845 Oct 25 00:30 test-ocs-ci.log",
            "title": "Log Files"
        },
        {
            "location": "/ocs_kvm/#remote-webconsole-support",
            "text": "The cluster create command outputs webconsole information which should look something\nlike the first entry below.  This information needs to be added to your /etc/hosts file\nor MacOS equivalent defining the IP Address of the KVM host server.  You must generate\nthe companion  oauth  definition as shown below following the same pattern.  <ip kvm host server> console-openshift-console.apps.test-ocp4-5.tt.testing oauth-openshift.apps.test-ocp4-5.tt.testing  The browser should prompt you to login to the OCP cluster.  The user name is  kubeadmin  and\nthe password is located in the file  /auth/kubeadmin-password .",
            "title": "Remote Webconsole Support"
        },
        {
            "location": "/ocs_kvm/#chrontab-automation",
            "text": "The following two files have been provided:   chron-ocs.sh   test-chron-ocs.sh   The  chron-ocs.sh  script is the master chrontab commandline script.  It is located\nin the  scripts/helper  directory.  The  test-chron-ocs.sh  script is invoked by chron-ocs.sh and provides the\nend-to-end OCP/OCS command flow.  Presently, this script invokes tier tests\n2, 3, 4, 4a, 4b and 4c.  You can limit the tests to a subset by editing this file.\nThis file is located in the  samples  directory.   To setup chrontab automation, you must:   Create  test  user account with sudo authority and login to it  git clone this project in $HOME and invoke  scripts/helper/set-passwordless-sudo.sh  Place the required files defined by the ocs-upi-kvm project in $HOME  Copy the two chron scripts listed above to $HOME  Edit the four lines below in  test-chron-ocs.sh :   export RHID_USERNAME=<Your RedHat Subscription id>\nexport RHID_PASSWORD=<RedHat Subscription password>\nexport OCP_VERSION=4.5\nexport IMAGES_PATH=/home/libvirt/images   Invoke  crontab -e  and enter the following two lines:   SHELL=/bin/bash \n0 0 * * * ~/chron-ocs.sh  The example above will invoke chron-ocs.sh every 24 hours at midnight local time.  Log files are written to the  logs  directory under the user's home directory.",
            "title": "Chrontab Automation"
        },
        {
            "location": "/playbook/",
            "text": "Introduction\n\n\nThe \nplaybooks\n are used for installation of OCP on Power and other post install customizations.\nThe playbooks are used with \nPowerVS\n, \nPowerVC\n\nand \nKVM\n projects.\n\n\nAssumptions\n\n\n\n\nA bastion/helper node is already created where the playbooks would run.\n\n\nThe required services are configured on the bastion/helper node using \nhelpernode playbook\n.\n\n\nThe cluster nodes are already created.\n\n\n\n\nBastion HA setup\n\n\nWe can have multiple bastion nodes as part of the OpenShift 4.X setup. Ensure that all the required services are configured on all the bastion nodes. Also, keepalived service should be configured and running.\n\n\nTo use this playbook for bastion HA you need to:\n1. Run helpernode playbook with \nHA configurations\n.\n1. Use \nbastion_vip\n variable with keepalived vrrp address.\n1. Add \nbastion\n host group entries with all bastion nodes in \nexamples/inventory\n.\n\n\nThe OpenShift install commands will always run on the first bastion. Additional services such as squid proxy, chrony, etc. will be configured on all nodes. The auth directory and ignition files will be available on all nodes once the installation complete.\n\n\nSet up the required variables\n\n\nMake use of the sample file at \nexamples/install_vars.yaml\n. Modify the values as per your cluster.\n\n\ncp examples/install_vars.yaml .\n\n\n\n\nUse install_vars.yaml\n\n\nThis section sets the variables for the install-config.yaml template file.\n\n\ninstall_config:\n   cluster_domain: < Cluster domain name. Match to the baseDomain in install-config.yaml.>\n   cluster_id: < Cluster identifier. Match to the metadata.name in install-config.yaml.>\n   pull_secret: '<pull-secret json content>'\n   public_ssh_key: '<SSH public key content to access the cluster nodes>'\n\n\n\n\nBelow variables will be used by the OCP install playbook.\n\n\nworkdir: <Directory to use for creating OCP configs>\nstorage_type: <Storage type used in the cluster. Eg: nfs (Note: Currently NFS provisioner is not configured using this playbook. This variable is only used for setting up image registry to EmptyDir if storage_type is not nfs)>\nlog_level: <Option --log-level in openshift-install commands. Default is 'info'>\nrelease_image_override: '<This is set to OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE while creating ign files. If you are using internal artifactory then ensure that you have added auth key to the pull_secret>'\nrhcos_kernel_options: <List of kernel options for RHCOS nodes eg: [\"slub_max_order=0\",\"loglevel=7\"]>\n\n\n\n\nSetting up inventory\n\n\nMake use of sample file at \nexamples/inventory\n. Modify the host values as per your cluster.\n\n\ncp examples/inventory .\n\n\n\n\nRun the playbook\n\n\nOnce you have configured the vars & inventory file, run the install playbook using:\n\n\nansible-playbook  -i inventory -e @install_vars.yaml playbooks/install.yaml\n\n\n\n\nLicense\n\n\nSee LICENCE.txt",
            "title": "Playbook"
        },
        {
            "location": "/playbook/#introduction",
            "text": "The  playbooks  are used for installation of OCP on Power and other post install customizations.\nThe playbooks are used with  PowerVS ,  PowerVC \nand  KVM  projects.",
            "title": "Introduction"
        },
        {
            "location": "/playbook/#assumptions",
            "text": "A bastion/helper node is already created where the playbooks would run.  The required services are configured on the bastion/helper node using  helpernode playbook .  The cluster nodes are already created.",
            "title": "Assumptions"
        },
        {
            "location": "/playbook/#bastion-ha-setup",
            "text": "We can have multiple bastion nodes as part of the OpenShift 4.X setup. Ensure that all the required services are configured on all the bastion nodes. Also, keepalived service should be configured and running.  To use this playbook for bastion HA you need to:\n1. Run helpernode playbook with  HA configurations .\n1. Use  bastion_vip  variable with keepalived vrrp address.\n1. Add  bastion  host group entries with all bastion nodes in  examples/inventory .  The OpenShift install commands will always run on the first bastion. Additional services such as squid proxy, chrony, etc. will be configured on all nodes. The auth directory and ignition files will be available on all nodes once the installation complete.",
            "title": "Bastion HA setup"
        },
        {
            "location": "/playbook/#set-up-the-required-variables",
            "text": "Make use of the sample file at  examples/install_vars.yaml . Modify the values as per your cluster.  cp examples/install_vars.yaml .",
            "title": "Set up the required variables"
        },
        {
            "location": "/playbook/#use-install_varsyaml",
            "text": "This section sets the variables for the install-config.yaml template file.  install_config:\n   cluster_domain: < Cluster domain name. Match to the baseDomain in install-config.yaml.>\n   cluster_id: < Cluster identifier. Match to the metadata.name in install-config.yaml.>\n   pull_secret: '<pull-secret json content>'\n   public_ssh_key: '<SSH public key content to access the cluster nodes>'  Below variables will be used by the OCP install playbook.  workdir: <Directory to use for creating OCP configs>\nstorage_type: <Storage type used in the cluster. Eg: nfs (Note: Currently NFS provisioner is not configured using this playbook. This variable is only used for setting up image registry to EmptyDir if storage_type is not nfs)>\nlog_level: <Option --log-level in openshift-install commands. Default is 'info'>\nrelease_image_override: '<This is set to OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE while creating ign files. If you are using internal artifactory then ensure that you have added auth key to the pull_secret>'\nrhcos_kernel_options: <List of kernel options for RHCOS nodes eg: [\"slub_max_order=0\",\"loglevel=7\"]>",
            "title": "Use install_vars.yaml"
        },
        {
            "location": "/playbook/#setting-up-inventory",
            "text": "Make use of sample file at  examples/inventory . Modify the host values as per your cluster.  cp examples/inventory .",
            "title": "Setting up inventory"
        },
        {
            "location": "/playbook/#run-the-playbook",
            "text": "Once you have configured the vars & inventory file, run the install playbook using:  ansible-playbook  -i inventory -e @install_vars.yaml playbooks/install.yaml",
            "title": "Run the playbook"
        },
        {
            "location": "/playbook/#license",
            "text": "See LICENCE.txt",
            "title": "License"
        },
        {
            "location": "/powervm/",
            "text": "Table of Contents\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nPre-requisites\n\n\nOperating Systems\n\n\nPackages\n\n\n\n\n\n\nImage and LPAR requirements\n\n\nOCP Install\n\n\nContributing\n\n\n\n\nIntroduction\n\n\nThis repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on PowerVM LPARs.\nThis assumes PowerVC is used as the IaaS layer for managing the PowerVM LPARs.\n\n\nIf you are using standalone PowerVM please take a look at the \nfollowing quickstart guide\n\nwhich uses the \nansible playbook\n to setup helper node (bastion) for OCP deployment.\n\n\nThis project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC.\n\n\nRun this code from either Mac or Linux (Intel) system. For automating setup, see\n\nOCP-UPI-PowerVM\n\n\n:heavy_exclamation_mark: \nThis automation is intended for test/development purposes only and there is no formal support. For bugs/enhancement requests etc. please open a GitHub issue\n\n\nPre-requisites\n\n\nYou need to identify a remote client machine for running the automation. This could be your laptop or a VM.\n\n\nOperating Systems\n\n\nThis code has been tested on the following x86-64 based Operating Systems:\n - Linux\n - Mac OSX (Darwin)\n\n\nPackages\n\n\nInstall the below required packages on the client machine.\n\n\n\n\nGit\n: Please refer to the \nlink\n for instructions\non installing the latest Git.\n\n\nTerraform >= 0.13.0\n: Please refer to the \nlink\n for instructions on installing Terraform. For validating the version run \nterraform version\n command after install.\n\n\nPublic internet connection for all the nodes to download the playbooks and images as part of the setup process.\n\n\n\n\nImage and LPAR requirements\n\n\nYou'll need to create RedHat CoreOS (RHCOS) and RHEL 8.0 (or later) image in PowerVC. For RHCOS image creation, follow the steps mentioned\nin the following \ndoc\n.\n\n\nFollowing are the recommended LPAR configs for OpenShift nodes that will be deployed with RHCOS image.\n- Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk.\n\n\nPowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be \n16\n (\n2 vCPUs x 8 SMT\n)\n\n\nThis config is suitable for majority of the scenarios\n\n- Worker - 2 vCPUs, 16GB RAM, 120 GB Disk\n\n\nIncrease worker vCPUs, RAM and Disk based on application requirements\n\n\nFollowing is the recommended LPAR config for the helper node that will be deployed with RHEL 8.0 (or later) image.\n- Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk\n\n\nOCP Install\n\n\nFollow these \nquickstart\n steps to kickstart OCP installation on PowerVM LPARs managed via PowerVC\n\n\nContributing\n\n\nPlease see the \ncontributing doc\n for more details.\nPRs are most welcome !!",
            "title": "Powervm"
        },
        {
            "location": "/powervm/#table-of-contents",
            "text": "Table of Contents  Introduction  Pre-requisites  Operating Systems  Packages    Image and LPAR requirements  OCP Install  Contributing",
            "title": "Table of Contents"
        },
        {
            "location": "/powervm/#introduction",
            "text": "This repo contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on PowerVM LPARs.\nThis assumes PowerVC is used as the IaaS layer for managing the PowerVM LPARs.  If you are using standalone PowerVM please take a look at the  following quickstart guide \nwhich uses the  ansible playbook  to setup helper node (bastion) for OCP deployment.  This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC.  Run this code from either Mac or Linux (Intel) system. For automating setup, see OCP-UPI-PowerVM  :heavy_exclamation_mark:  This automation is intended for test/development purposes only and there is no formal support. For bugs/enhancement requests etc. please open a GitHub issue",
            "title": "Introduction"
        },
        {
            "location": "/powervm/#pre-requisites",
            "text": "You need to identify a remote client machine for running the automation. This could be your laptop or a VM.",
            "title": "Pre-requisites"
        },
        {
            "location": "/powervm/#operating-systems",
            "text": "This code has been tested on the following x86-64 based Operating Systems:\n - Linux\n - Mac OSX (Darwin)",
            "title": "Operating Systems"
        },
        {
            "location": "/powervm/#packages",
            "text": "Install the below required packages on the client machine.   Git : Please refer to the  link  for instructions\non installing the latest Git.  Terraform >= 0.13.0 : Please refer to the  link  for instructions on installing Terraform. For validating the version run  terraform version  command after install.  Public internet connection for all the nodes to download the playbooks and images as part of the setup process.",
            "title": "Packages"
        },
        {
            "location": "/powervm/#image-and-lpar-requirements",
            "text": "You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.0 (or later) image in PowerVC. For RHCOS image creation, follow the steps mentioned\nin the following  doc .  Following are the recommended LPAR configs for OpenShift nodes that will be deployed with RHCOS image.\n- Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk.  PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be  16  ( 2 vCPUs x 8 SMT )  This config is suitable for majority of the scenarios \n- Worker - 2 vCPUs, 16GB RAM, 120 GB Disk  Increase worker vCPUs, RAM and Disk based on application requirements  Following is the recommended LPAR config for the helper node that will be deployed with RHEL 8.0 (or later) image.\n- Helper node (bastion) - 2vCPUs, 16GB RAM, 200 GB Disk",
            "title": "Image and LPAR requirements"
        },
        {
            "location": "/powervm/#ocp-install",
            "text": "Follow these  quickstart  steps to kickstart OCP installation on PowerVM LPARs managed via PowerVC",
            "title": "OCP Install"
        },
        {
            "location": "/powervm/#contributing",
            "text": "Please see the  contributing doc  for more details.\nPRs are most welcome !!",
            "title": "Contributing"
        },
        {
            "location": "/powervs/",
            "text": "Table of Contents\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nAutomation Host Prerequisites\n\n\nPowerVS Prerequisites\n\n\nOCP Install\n\n\nContributing\n\n\n\n\nIntroduction\n\n\nPowerVS\n contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on \nIBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud\n.\n\n\nThis project leverages the helpernode \nansible playbook\n internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS).\n\n\n:heavy_exclamation_mark: \nFor bugs/enhancement requests etc. please open a GitHub issue\n\n\nFor general PowerVS usage instructions please refer to the following links:\n- https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started\n- https://www.youtube.com/watch?v=RywSfXT_LLs\n- https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv\n\n\n:information_source: \nThis branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - {release-4.5, release-4.6 ...} and follow the docs.\n\n\nAutomation Host Prerequisites\n\n\nThe automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems:\n- Mac OSX (Darwin)\n- Linux (x86_64)\n- Windows 10\n\n\nFollow the \nguide\n to complete the prerequisites.\n\n\nPowerVS Prerequisites\n\n\nFollow the \nguide\n to complete the PowerVS prerequisites.\n\n\nOCP Install\n\n\nFollow the \nquickstart\n guide for OCP installation on PowerVS.\n\n\nContributing\n\n\nPlease see the \ncontributing doc\n for more details.\nPRs are most welcome !!",
            "title": "Powervs"
        },
        {
            "location": "/powervs/#table-of-contents",
            "text": "Table of Contents  Introduction  Automation Host Prerequisites  PowerVS Prerequisites  OCP Install  Contributing",
            "title": "Table of Contents"
        },
        {
            "location": "/powervs/#introduction",
            "text": "PowerVS  contains Terraform templates to help deployment of OpenShift Container Platform (OCP) 4.x on  IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud .  This project leverages the helpernode  ansible playbook  internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS).  :heavy_exclamation_mark:  For bugs/enhancement requests etc. please open a GitHub issue  For general PowerVS usage instructions please refer to the following links:\n- https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started\n- https://www.youtube.com/watch?v=RywSfXT_LLs\n- https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv  :information_source:  This branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - {release-4.5, release-4.6 ...} and follow the docs.",
            "title": "Introduction"
        },
        {
            "location": "/powervs/#automation-host-prerequisites",
            "text": "The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems:\n- Mac OSX (Darwin)\n- Linux (x86_64)\n- Windows 10  Follow the  guide  to complete the prerequisites.",
            "title": "Automation Host Prerequisites"
        },
        {
            "location": "/powervs/#powervs-prerequisites",
            "text": "Follow the  guide  to complete the PowerVS prerequisites.",
            "title": "PowerVS Prerequisites"
        },
        {
            "location": "/powervs/#ocp-install",
            "text": "Follow the  quickstart  guide for OCP installation on PowerVS.",
            "title": "OCP Install"
        },
        {
            "location": "/powervs/#contributing",
            "text": "Please see the  contributing doc  for more details.\nPRs are most welcome !!",
            "title": "Contributing"
        },
        {
            "location": "/terraform/",
            "text": "Terraform Provider\n\n\n\n\nWebsite: https://www.terraform.io\n\n\n\n\nMailing list: \nGoogle Groups\n\n\n\n\nRequirements\n\n\n\n\nTerraform\n 0.10.1+\n\n\nGo\n 1.13 (to build the provider plugin)\n\n\n\n\nBuilding The Provider\n\n\nClone repository to: \n$GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm\n\n\nmkdir -p $GOPATH/src/github.com/IBM-Cloud; cd $GOPATH/src/github.com/IBM-Cloud\ngit clone git@github.com:IBM-Cloud/terraform-provider-ibm.git\n\n\n\n\nEnter the provider directory and build the provider\n\n\ncd $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm\nmake build\n\n\n\n\nDocker Image For The Provider\n\n\nYou can also pull the docker image for the ibmcloud terraform provider :\n\n\ndocker pull ibmterraform/terraform-provider-ibm-docker\n\n\n\n\nUsing the provider\n\n\nIf you want to run \nTerraform with the IBM Cloud provider\n plugin on your system, complete the following steps:\n\n\n\n\n\n\nDownload and install Terraform for your system\n. \n\n\n\n\n\n\nDownload the IBM Cloud provider plugin for Terraform\n.\n\n\n\n\n\n\nUnzip the release archive to extract the plugin binary (\nterraform-provider-ibm_vX.Y.Z\n).\n\n\n\n\n\n\nMove the binary into the Terraform \nplugins directory\n for the platform.\n\n\n\n\nLinux/Unix/OS X: \n~/.terraform.d/plugins\n\n\nWindows: \n%APPDATA%\\terraform.d\\plugins\n\n\n\n\n\n\n\n\nExport API credential tokens as environment variables. This can either be \nIBM Cloud API keys\n or Softlayer API keys and usernames, depending on the resources you are provisioning.\n\n\n\n\n\n\nexport IC_API_KEY=\"IBM Cloud API Key\"\nexport IAAS_CLASSIC_API_KEY=\"IBM Cloud Classic Infrastructure API Key\"\nexport IAAS_CLASSIC_USERNAME=\"IBM Cloud Classic Infrastructure username associated with Classic Infrastructure API KEY\".\n\n\n\n\n\n\nAdd the plug-in provider to the Terraform configuration file.\n\n\n\n\nprovider \"ibm\" {}\n\n\n\n\nSee the \nofficial documentation\n for more details on using the IBM provider.\n\n\nDeveloping the Provider\n\n\nIf you wish to work on the provider, you'll first need \nGo\n installed on your machine (version 1.8+ is \nrequired\n). You'll also need to correctly setup a \nGOPATH\n, as well as adding \n$GOPATH/bin\n to your \n$PATH\n.\n\n\nTo compile the provider, run \nmake build\n. This will build the provider and put the provider binary in the \n$GOPATH/bin\n directory.\n\n\nmake build\n...\n$GOPATH/bin/terraform-provider-ibm\n...\n\n\n\n\nIn order to test the provider, you can simply run \nmake test\n.\n\n\nmake test\n\n\n\n\nIn order to run the full suite of Acceptance tests, run \nmake testacc\n.\n\n\nNote:\n Acceptance tests create real resources, and often cost money to run.\n\n\nmake testacc\n\n\n\n\nIn order to run a particular Acceptance test, export the variable \nTESTARGS\n. For example\n\n\nexport TESTARGS=\"-run TestAccIBMNetworkVlan_Basic\"\n\n\n\n\nIssuing \nmake testacc\n will now run the testcase with names matching \nTestAccIBMNetworkVlan_Basic\n. This particular testcase is present in\n\nibm/resource_ibm_network_vlan_test.go\n\n\nYou will also need to export the following environment variables for running the Acceptance tests.\n\n \nIC_API_KEY\n- The IBM Cloud API Key\n\n \nIAAS_CLASSIC_API_KEY\n - The IBM Cloud Classic Infrastructure API Key\n* \nIAAS_CLASSIC_USERNAME\n - The IBM Cloud Classic Infrastructure username associated with the Classic InfrastAPI Key.\n\n\nAdditional environment variables may be required depending on the tests being run. Check console log for warning messages about required variables. \n\n\nIBM Cloud Ansible Modules\n\n\nAn implementation of generated Ansible modules using the\n\nIBM Cloud Terraform Provider\n.\n\n\nPrerequisites\n\n\n\n\n\n\nInstall \nPython3\n\n\n\n\n\n\nRedHat Ansible\n 2.8+\n\n\npip install \"ansible>=2.8.0\"\n\n\n\n\n\n\nInstall\n\n\n\n\n\n\nDownload IBM Cloud Ansible modules from \nrelease page\n\n\n\n\n\n\nExtract module archive.\n\n\nunzip ibmcloud_ansible_modules.zip\n\n\n\n\n\n\nAdd modules and module_utils to the \nAnsible search path\n. E.g.:\n\n\n```\ncp build/modules/\n $HOME/.ansible/plugins/modules/.\ncp build/module_utils/\n $HOME/.ansible/plugins/module_utils/.\n\n\n```\n\n\n\n\n\n\nExample Projects\n\n\n\n\n\n\nVPC Virtual Server Instance\n\n\n\n\n\n\nPower Virtual Server Instance",
            "title": "Terraform"
        },
        {
            "location": "/terraform/#terraform-provider",
            "text": "Website: https://www.terraform.io   Mailing list:  Google Groups",
            "title": "Terraform Provider"
        },
        {
            "location": "/terraform/#requirements",
            "text": "Terraform  0.10.1+  Go  1.13 (to build the provider plugin)",
            "title": "Requirements"
        },
        {
            "location": "/terraform/#building-the-provider",
            "text": "Clone repository to:  $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm  mkdir -p $GOPATH/src/github.com/IBM-Cloud; cd $GOPATH/src/github.com/IBM-Cloud\ngit clone git@github.com:IBM-Cloud/terraform-provider-ibm.git  Enter the provider directory and build the provider  cd $GOPATH/src/github.com/IBM-Cloud/terraform-provider-ibm\nmake build",
            "title": "Building The Provider"
        },
        {
            "location": "/terraform/#docker-image-for-the-provider",
            "text": "You can also pull the docker image for the ibmcloud terraform provider :  docker pull ibmterraform/terraform-provider-ibm-docker",
            "title": "Docker Image For The Provider"
        },
        {
            "location": "/terraform/#using-the-provider",
            "text": "If you want to run  Terraform with the IBM Cloud provider  plugin on your system, complete the following steps:    Download and install Terraform for your system .     Download the IBM Cloud provider plugin for Terraform .    Unzip the release archive to extract the plugin binary ( terraform-provider-ibm_vX.Y.Z ).    Move the binary into the Terraform  plugins directory  for the platform.   Linux/Unix/OS X:  ~/.terraform.d/plugins  Windows:  %APPDATA%\\terraform.d\\plugins     Export API credential tokens as environment variables. This can either be  IBM Cloud API keys  or Softlayer API keys and usernames, depending on the resources you are provisioning.    export IC_API_KEY=\"IBM Cloud API Key\"\nexport IAAS_CLASSIC_API_KEY=\"IBM Cloud Classic Infrastructure API Key\"\nexport IAAS_CLASSIC_USERNAME=\"IBM Cloud Classic Infrastructure username associated with Classic Infrastructure API KEY\".   Add the plug-in provider to the Terraform configuration file.   provider \"ibm\" {}  See the  official documentation  for more details on using the IBM provider.",
            "title": "Using the provider"
        },
        {
            "location": "/terraform/#developing-the-provider",
            "text": "If you wish to work on the provider, you'll first need  Go  installed on your machine (version 1.8+ is  required ). You'll also need to correctly setup a  GOPATH , as well as adding  $GOPATH/bin  to your  $PATH .  To compile the provider, run  make build . This will build the provider and put the provider binary in the  $GOPATH/bin  directory.  make build\n...\n$GOPATH/bin/terraform-provider-ibm\n...  In order to test the provider, you can simply run  make test .  make test  In order to run the full suite of Acceptance tests, run  make testacc .  Note:  Acceptance tests create real resources, and often cost money to run.  make testacc  In order to run a particular Acceptance test, export the variable  TESTARGS . For example  export TESTARGS=\"-run TestAccIBMNetworkVlan_Basic\"  Issuing  make testacc  will now run the testcase with names matching  TestAccIBMNetworkVlan_Basic . This particular testcase is present in ibm/resource_ibm_network_vlan_test.go  You will also need to export the following environment variables for running the Acceptance tests.   IC_API_KEY - The IBM Cloud API Key   IAAS_CLASSIC_API_KEY  - The IBM Cloud Classic Infrastructure API Key\n*  IAAS_CLASSIC_USERNAME  - The IBM Cloud Classic Infrastructure username associated with the Classic InfrastAPI Key.  Additional environment variables may be required depending on the tests being run. Check console log for warning messages about required variables.",
            "title": "Developing the Provider"
        },
        {
            "location": "/terraform/#ibm-cloud-ansible-modules",
            "text": "An implementation of generated Ansible modules using the IBM Cloud Terraform Provider .",
            "title": "IBM Cloud Ansible Modules"
        },
        {
            "location": "/terraform/#prerequisites",
            "text": "Install  Python3    RedHat Ansible  2.8+  pip install \"ansible>=2.8.0\"",
            "title": "Prerequisites"
        },
        {
            "location": "/terraform/#install",
            "text": "Download IBM Cloud Ansible modules from  release page    Extract module archive.  unzip ibmcloud_ansible_modules.zip    Add modules and module_utils to the  Ansible search path . E.g.:  ```\ncp build/modules/  $HOME/.ansible/plugins/modules/.\ncp build/module_utils/  $HOME/.ansible/plugins/module_utils/.  ```",
            "title": "Install"
        },
        {
            "location": "/terraform/#example-projects",
            "text": "VPC Virtual Server Instance    Power Virtual Server Instance",
            "title": "Example Projects"
        }
    ]
}